CREATE MODEL('o4-moose', 'o4-mini', 'openai', {"context_window": 200000, "max_output_tokens": 32000});

SELECT id,
       llm_complete(
           {'model_name': 'mini'},
    {'prompt': 'For the next {job_description} assume the role of a professional resume writer for
     technical positions given a job description and the !RESUME! below:

    !RESUME! <<

    personal_information:
      name: "William"
      surname: "Clark"
      country: "United States"
      city: "Durham"
      phone_prefix: "+1"
      phone: "919-308-7561"
      email: "wfclark5@gmail.com"
      github: "https://github.com/wfclark5"
      linkedin: "https://www.linkedin.com/in/william-clark-5b621313a/"
      address: "912 Camden Ave"
      date_of_birth: "1990-01-01"
      pronouns: "he/him"


    education_details:
      - education_level: "Master of Science"
        institution: "East Carolina University"
        field_of_study: "Computer Science"
        year_of_completion: 0
        start_date: ""
        final_evaluation_grade: ""
      - education_level: "Bachelor of Science"
        institution: "University of North Carolina at Asheville"
        field_of_study: "Atmospheric Sciences, Minor in Mathematics"
        year_of_completion: 0
        start_date: ""
        final_evaluation_grade: ""

    experience_details:
      - position: "Associate Director - AI Practice"
        company: "X by 2"
        employment_period: "2023 — Present"
        location: "Detroit, MI"
        industry: "Technology Consulting"
        key_responsibilities:
          - responsibility_1: "Expanded AI consulting and product development capabilities across integration, infrastructure, and operations."
          - responsibility_2: "Designed scalable analytics tooling ecosystems to streamline pipeline and model development."
          - responsibility_3: "Architected developer tooling ecosystem using DBT, Ray, Kubernetes, and Karpenter on AWS."
          - responsibility_4: "Implemented preconfigured environments via Coder, AWS, Kubernetes, and Terraform."
          - responsibility_5: "Integrated generative AI with Power Platform, AWS Lambda, ECS, and Azure OpenAI API."
          - responsibility_6: "Mentored engineers on AI, public cloud, and cloud-native best practices."
          - responsibility_7: "Built Bitbucket Pipelines for CI/CD to ensure traceable model and feature deployments."
        skills_acquired:
          - "Clinical Optimization for Emergency Departments"
          - "Predictive Analytics"
          - "AI Product Development"
          - "AI and ML Model Development"
          - "AI as a Service"
          - "Data Engineering"
          - "Inference API Development"
          - "HL7 FHIR Integration"
          - "Generative AI"
          - "Deep Learning"
          - "Transformer Models"
          - "Modular Architecture"
          - "Developer Onboarding and Training"
          - "Internal Developer Tooling"
          - "Developer Platforms"
          - "Cloud Development Environments"
          - "ML Feature Engineering"
          - "Model Tuning and Optimization"
          - "GPU Compute Environments"
          - "Ephemeral Compute Environments"
          - "Speech Processing and Summarization"
          - "HIPAA Compliance"
          - "Data Governance"
          - "Data Quality"
          - "Data Lineage"
          - "Data Modeling"
          - "Data Ingestion"
          - "DataOps"
          - "MLOps"
          - "CI/CD"
          - "Ray"
          - "DBT"
          - "AWS"
          - "Kubernetes"
          - "Terraform"
          - "Bitbucket Pipelines"
          - "AWS Lambda"
          - "AWS ECS"
          - "AWS S3"
          - "AWS Networking (Public and Private)"
          - "AWS IAM"
          - "AWS EKS"
          - "AWS Cost Management and Reporting"
          - "HELM"
          - "Karpenter"
          - "Coder"
          - "Azure AI Studio"
          - "ECS"
          - "Power Platform"
          - "Weights and Biases"
          - "DuckDB"
          - "Tensorflow"


      - position: "Associate Director - Client"
        company: "MESSA"
        employment_period: "2024 — 2024"
        location: "Lansing, MI"
        industry: "Healthcare"
        key_responsibilities:
          - responsibility_1: "Implemented secure, HIPAA-compliant Azure architecture across networking, security, and compute."
          - responsibility_2: "Integrated cross-subscription services using Azure Virtual WAN and VPN tunnels."
          - responsibility_3: "Created 20+ architectural diagrams and documentation for cloud hybridization."
          - responsibility_4: "Developed and applied Azure Policy and Microsoft Defender governance controls."
          - responsibility_5: "Implemented secure identity access with RBAC, Azure AD (Entra ID), and Managed Identities."
          - responsibility_6: "Deployed 150+ Azure resources via custom Terraform modules."
          - responsibility_7: "Enabled secure data access and audit logging for SQL, Storage, and Data Lake Gen2."
        skills_acquired:
          - "Hybrid Cloud Architecture"
          - "Azure HIPAA-compliant Architecture"
          - "Azure Networking"
          - "Azure Security"
          - "Azure Compute"
          - "Azure Storage"
          - "Azure Data Lake Gen2"
          - "Azure Data Factory"
          - "Azure Functions"
          - "Azure Virtual WAN"
          - "Azure Policy"
          - "Azure SQL (MSSQL)"
          - "Azure Defender"
          - "Azure Monitor"
          - "Azure Log Analytics"
          - "Event and Data Driven Architecture"
          - "Microsoft Entra ID"
          - "RBAC and Managed Identities"
          - "Third Party App Registration and Integration"
          - "Terraform"
          - "C#"
          - "Azure DevOps"
          - "Developer Onboarding"

      - position: "Senior Consultant - Client"
        company: "ForzaCare"
        employment_period: "2023 — 2024"
        location: "Nashville, TN"
        industry: "Healthcare"
        key_responsibilities:
          - responsibility_1: "Spearheaded construction and deployment of claims repricing engine harnessing Python, open-source libraries, Azure PostgreSQL, and Azure Kubernetes Service. The engine normalized, repriced, and regenerated EDI 837i/p claims based on B2B contracts."
          - responsibility_2: "Engineered 15+ near real-time data pipelines to facilitate efficient claims processing operations."
          - responsibility_3: "Authored 35 pages of comprehensive technical documentation and standard operating procedures to support system operations."
          - responsibility_4: "Played pivotal role in onboarding 6 new claims partners, contributing to ForzaCares initial revenue generation."
          - responsibility_5: "Developed an automated EDI pipeline to ensure seamless repricing workflows, writing outputs back to EDI formats for downstream partner consumption."
          - responsibility_6: "Created provider network platform by ingesting multi-format flat files (via SFTP) and near real-time EDI data."
          - responsibility_7: "Developed Salesforce integration with ingestion APIs for provider platform."
          - responsibility_8: "Implemented data quality checks and observability for data pipelines, ensuring high standards of data integrity."
          - responsibility_9: "Designed and implemented severity level notification and alerting system for data pipeline observability in MS Teams."
          - responsibility_10: "Created PowerBI dashboards to report out on stakeholder KPIs (such as revenue generated) and data quality metrics."

        skills_acquired:
          - "Azure Kubernetes Service"
          - "Azure PostgreSQL"
          - "Azure Storage"
          - "Azure DevOps"
          - "Airflow"
          - "Python"
          - "PostgreSQL"
          - "SFTP Integration"
          - "Salesforce ETL / ELT"
          - "Workers Compensation Claim Processing"
          - "Claims Adjudication"
          - "Claims Repricing"
          - "Claims Processing"
          - "Claims Data Normalization"
          - "Provider Networking and Contracting"
          - "Data Ingestion"
          - "Data Quality"
          - "Data Modeling"
          - "Data Visualization"
          - "Data Pipeline Observability"
          - "Notification and Alerting"
          - "Distributed Data Processing"
          - "EDI Conversion and Normalization"
          - "Standard Operating Procedures (SOPs)"
          - "Data Asset Technical Specifications"
          - "PowerBI"
          - "DuckDB"
          - "Developer Onboarding and Training"


      - position: "Technical Lead - Data Science Center of Excellence"
        company: "Blue Cross NC"
        employment_period: "2022 — 2023"
        location: "Durham, NC"
        industry: "Healthcare Insurance"
        key_responsibilities:
          - responsibility_1: "Drove the *Insights Activation* program—defined epics, stories, and timelines to modernise the DS-CoE analytics platform and SDLC."
          - responsibility_2: "Reduced cloud spend by $2.75M through model optimization and cost control."
          - responsibility_3: "Evaluated six MLOps vendor platforms including Ray and Prefect."
          - responsibility_4: "Orchestrated unified AI tooling workflows for analytics engineers."
          - responsibility_5: "Built dual-track CI/CD: • non-SageMaker analytics (R, custom Python) via CodeBuild/CodePipeline → EKS • SageMaker pipelines and endpoints with BYOC images, manual approval gates, and automatic ServiceNow change tickets."
          - responsibility_6: "Mentored data scientists and engineers on MLOps, CloudOps, DataOps best practices; ran training on Terraform, SageMaker, Prefect, and CI/CD pipelines."

        skills_acquired:
          - "AWS SageMaker"
          - "Cloud Cost Optimization"
          - "Ray"
          - "Prefect"
          - "CI/CD"
          - "Analytics Enablement"
          - "Data & Model Bias, Quality, Drift Monitoring"
          - "Feature Store Design & ML Lineage"
          - "ServiceNow ITIL Change-Control Automation"
          - "Control-M Job Scheduling Integration"
          - "AWS Accounts Architecture & Cost Guardrails"
          - "AWS Budgets, Cost Explorer & Infracost"
          - "AWS EKS, Karpenter & Istio Service Mesh"
          - "AWS SageMaker (Domain, Pipelines, Feature Store, Clarify)"
          - "AWS CodeBuild / CodePipeline"
          - "Argo CD & Helm Release Automation"
          - "Atlantis Terraform PR Automation"
          - "Terraform & Terragrunt Module Design"
          - "Kuberenetes API & Job Resource Creation"

      - position: "Senior Developer – Innovation Garage"
        company: "Blue Cross NC"
        employment_period: "2021 — 2022"
        location: "Durham, NC"
        industry: "Healthcare Insurance"
        key_responsibilities:
          - responsibility_1: "Served as a core member of a specialized team that designed, built, and maintained a state of the art transformer model factory for care management use cases."
          - responsibility_2: "Developed and deployed FastAPI micro-services and serverless endpoints (ECS/Fargate) with OAuth2 auth to serve model inference outputs."
          - responsibility_3: "Refactored model training and batch inference into an ephemeral Prefect-driven serverless orchestration framework"
          - responsibility_4: "Re-engineered the data preprocessing pipeline by replacing multiprocessing on a single compute instance with distributed chunking and tokenization using Dask, Fargate, and the Container Chaining pattern, boosting overall training and batch inference speed by roughly ten times."
          - responsibility_5: "Ensured MLOps best practices were built into orchestration pipeline such as experiment tracking, model versioning, and lineage."
          - responsibility_6: "Led the development of a universal payer-to-provider data exchange using HL7 FHIR in Epics App Orchard to write model risk scores into patient charts."
          - responsibility_7: "Authored within MkDocs documentation outlining the Transformer Model Factory’s architecture, usage, and publishing the Swagger API specifications used across the deep-learning model-factory library."
          - responsibility_8: "Developed and standardized reusable Terraform modules for secure, compliant deployments of AWS Infrastructure"
          - responsibility_9: "Maintained Jenkins CI/CD framework to build and deploy ECR containers from SCM for all Innovation Garage services."
        skills_acquired:
          - "Transformer-based Deep Learning Architecture"
          - "Weights & Biases for Experiment Tracking, Model Versioning, and Lineage"
          - "AWS ECS & Fargate for Compute and Execution"
          - "FHIR (R4), SMART on FHIR, HL7 Interoperability"
          - "Epic App Orchard Integration"
          - "K-Nearest Neighbors (KNN) Geospatial Analysis"
          - "Prefect Workflow Orchestration"
          - "Dask Distributed Processing"
          - "FastAPI API Development"
          - "AWS Fargate & ECS"
          - "Auto Scaling Groups for GPU Workloads"
          - "Terraform Module Design"
          - "HIPAA-Compliant Cloud Infrastructure"
          - "Infrastructure as Code (IaC)"
          - "AWS Cognito Authentication"
          - "CloudWatch & OpenTelemetry Monitoring"
          - "Grafana Dashboards"
          - "AWS Cost Explorer & Resource Tagging"
          - "S3 & KMS Configuration"
          - "MkDocs (Material) for Architecture Documentation"
          - "Jenkins CI/CD Maintenance"
          - "API Publishing & Internal Documentation"
          - "Route53 Hosted Zone Management"
          - "ALB Listener Rules & Target Group Routing"
          - "VPC, Security Group, and Subnet Configuration"

      - position: "Senior Data Engineer – Strategic Data Management"
        company: "Blue Cross NC"
        employment_period: "2020 — 2021"
        location: "Durham, NC"
        industry: "Healthcare Insurance"
        key_responsibilities:
          - responsibility_1: "Delivered 50+ enterprise data-lake and warehouse asset pipelines spanning behavioural, clinical, and financial domains."
          - responsibility_2: "Authored and enforced a team SOP that standardised Agile cadence, PR etiquette, and definition-of-done."
          - responsibility_3: "Established GitHub branching, commit-message, and Jira-linking standards for full traceability."
          - responsibility_4: "Created an n-layer stored-procedure framework with logging and error-handling."
          - responsibility_5: "Automated CI/CD: Jenkins → GitHub → Amazon EKS / Argo CD with ServiceNow change-ticket integration."
          - responsibility_6: "Implemented Control-M scheduling and run-books, accelerating QA→UAT→Prod promotion."
          - responsibility_7: "On-boarded new developers via a Confluence handbook covering tech-spec templates and data-model versioning."
        skills_acquired:
          - "Data Warehouse and Data Lake Development"
          - "Stored Procedure and SQL Development"
          - "Apache Spark Development"
          - "Data Architecture and Modeling"
          - "Data Quality Assurance"
          - "ServiceNow ITIL Change Management"
          - "Incident Resolution and Management"
          - "Control-M Workload Automation"
          - "SQL & Stored-Procedure Frameworks"
          - "Data Governance & Lineage"
          - "Confluence Knowledge-Base Creation"
          - "Developer Onboarding & Training"
          - "Health-Insurance Data Management (Commercial & Medicare)"
          - "Behavioral Health Analytics"
          - "Advanced Kidney Care Analytics"
          - "Value-Based Care Analytics"
          - "Care Management  Analytics"
          - "Societal Drivers of Health (SDOH) Analytics"
          - "Website Interaction Analytics (User Touch Points)"
          - "Customer-Service Feedback Survey Analytics (Voice of Consumer)"
          - "Claims, Provider & Member Data Modeling"
          - "Technical Documentation Authoring"
          - "DataOps & Agile Delivery"
          - "Jenkins / Argo CD / GitHub"
          - "Amazon EKS & Kubernetes"

      - position: "IT Analyst – Duke Center for Autism and Brain Development"
        company: "Duke University School of Medicine"
        employment_period: "2017 — 2019"
        location: "Durham, NC"
        industry: "Healthcare Research"
        key_responsibilities:
          - responsibility_1: "Built HIPAA-compliant analytics pipelines from Epic and RedCap data."
          - responsibility_2: "Created clinical measure forms with RedCap and SurveyMonkey."
          - responsibility_3: "Developed MERN web app to track adverse treatment events."
          - responsibility_4: "Collaborated with clinicians on integrating AI tools into research workflows."
        skills_acquired:
          - "HIPAA Compliance"
          - "IRB Compliance"
          - "Pediatric Psychiatry"
          - "Clinical Research"
          - "Clinical Trial Data Management"
          - "Clinical Measure Development"
          - "Clinical Analytics"
          - "Clinical Workflow Integration"
          - "Data Pipeline Development"
          - "Biometric Signal Analysis "
          - "Computer Vision"
          - "Swift Programming"
          - "SAS"
          - "MERN Stack"
          - "Epic"
          - "RedCap"
          - "SurveyMonkey"

      - position: "Geospatial Application Analyst I"
        company: "National Environmental Modeling and Analysis Center"
        employment_period: "2014 — 2017"
        location: "Asheville, NC"
        industry: "Environmental Analytics"
        key_responsibilities:
          - responsibility_1: "Created real-time geospatial models using Landsat and LiDAR data."
          - responsibility_2: "Hosted predictive models via AWS Lambda for OpenLayers visualization."
          - responsibility_3: "Engineered catastrophe models for extreme weather scenarios."
          - responsibility_4: "Developed web-based tools for federal and state adaptation planning."
        skills_acquired:
          - "Climate, Weather, and Environmental Modeling"
          - "Public - Private Partnership Creation"
          - "Stakeholder-Facing Technical Writing"
          - "Copy Writing"
          - "Geospatial Analysis and Modeling"
          - "Satellite Imagery Analysis"
          - "In-Situ Data Collection and Analysis"
          - "Real-time and Batch ETL"
          - "Big Data Processing"
          - "Point Cloud Processing"
          - "3D Feature Extraction and Segmenetation"
          - "3D Modeling and Visualization"
          - "Application and API Development"
          - "Map Tile Generation and Serving"
          - "Urban Prediction Modeling"
          - "Agricultural Modeling"
          - "Epidemiological Modeling (Vector-borne diseases)"
          - "Catastrophe Modeling"
          - "Data Visualization"
          - "ArcGIS"
          - "QGIS"
          - "SQL"
          - "PostGIS"
          - "Python"
          - "JavaScript"
          - "Drupal"
          - "AWS"
          - "Version Control"

    achievements:
      - name: "Herbert Stout GIS Excellence Award"
        description: "Recognized for outstanding geospatial analytics contributions."
      - name: "NCAUG Undergraduate Scholarship"
        description: "Awarded based on academic merit in GIS and environmental studies."

    languages:
      - language: "English"
        proficiency: "Native"

    interests:
      - "Healthcare analytics"
      - "Geospatial modeling"
      - "Machine learning"
      - "Generative AI"
      - "Cloud Infrastructure"
      - "Cloud-native development"
      - "Data engineering"
      - "MLOps"
      - "Kubernetes"
      - "DataOps"

    availability:
      notice_period: "2 weeks"

    salary_expectations:
      salary_range_usd: "180000 - 200000"
>>
##### Resume Creation Rules:
 1) create an ATS friendly resume and output it in Markdown.
 2) match the job description to the qualifications as best as possible.
 3) Dont ask clarifying questions ONLY output the response
 Finally) Provide a JSON response:
 {"tuples": [<curated_ats_friendly_resume_1>, <curated_resume_2>, ...]}
'},
{'job_description': regexp_replace(
  regexp_replace(description, '[^a-zA-Z0-9]+', ' ', 'g'),
  '\\s+', ' ', 'g'
)}
) as resume_llm
FROM main.jobs
where description is not null;